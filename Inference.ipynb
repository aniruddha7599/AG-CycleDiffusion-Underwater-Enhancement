{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b4b430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading checkpoint from D:\\DAIICT\\Sem 3\\Major Project 1\\AG-CycleDiffusion\\checkpoints\\checkpoint_step_60000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aniruddha shinde\\AppData\\Local\\Temp\\ipykernel_25504\\1725203380.py:159: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(cfg.CHECKPOINT_PATH, map_location=cfg.DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Generating translated image...\n",
      "‚úÖ Saved output to D:\\DAIICT\\Sem 3\\Major Project 1\\AG-CycleDiffusion\\inferred_samples\\LFT_5424_translated.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from PIL import Image\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "# =================================================================================\n",
    "# 1. Configuration & Hyperparameters\n",
    "# =================================================================================\n",
    "class Config:\n",
    "    \"\"\"Configuration class for model hyperparameters and paths.\"\"\"\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # --- Checkpoint Path ---\n",
    "    CHECKPOINT_PATH = \"D:\\DAIICT\\Sem 3\\Major Project 1\\AG-CycleDiffusion\\checkpoints\\checkpoint_step_60000.pth\"\n",
    "    \n",
    "    # --- Image Parameters ---\n",
    "    IMG_SIZE = 256\n",
    "    \n",
    "    # --- Diffusion Hyperparameters ---\n",
    "    TIMESTEPS = 200\n",
    "    \n",
    "    # --- EMA Decay (for loading) ---\n",
    "    EMA_DECAY = 0.999\n",
    "\n",
    "    # --- Sampling ---\n",
    "    SAMPLE_DIR = \"D:\\\\DAIICT\\\\Sem 3\\\\Major Project 1\\\\AG-CycleDiffusion\\\\inferred_samples\"\n",
    "    N_STEPS = 25  # Number of sampling steps (fewer for faster inference)\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.SAMPLE_DIR, exist_ok=True)\n",
    "\n",
    "# =================================================================================\n",
    "# 2. Diffusion Logic & Helpers\n",
    "# =================================================================================\n",
    "def cosine_beta_schedule(timesteps, s=0.008, device=cfg.DEVICE):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, device=device)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return betas.clamp(1e-6, 0.999)\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, timesteps=cfg.TIMESTEPS, device=cfg.DEVICE):\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        self.betas = cosine_beta_schedule(timesteps, device=device)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, n, condition_tensor, n_steps=cfg.N_STEPS):\n",
    "        model.eval()\n",
    "        x_t = torch.randn((n, 3, cfg.IMG_SIZE, cfg.IMG_SIZE), device=self.device)\n",
    "        ts_vec = torch.linspace(self.timesteps - 1, 0, n_steps + 1).long().to(self.device)\n",
    "        for i in range(n_steps):\n",
    "            t = ts_vec[i].expand(n)\n",
    "            pred_x0 = model(x_t, t, condition_tensor)\n",
    "            \n",
    "            alpha_cumprod = self.alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "            alpha_cumprod_prev = self.alphas_cumprod[ts_vec[i + 1]].view(-1, 1, 1, 1) if ts_vec[i + 1] >= 0 else torch.ones_like(alpha_cumprod)\n",
    "            \n",
    "            pred_noise = (x_t - torch.sqrt(alpha_cumprod) * pred_x0) / torch.sqrt(1. - alpha_cumprod)\n",
    "            dir_xt = torch.sqrt(1. - alpha_cumprod_prev) * pred_noise\n",
    "            x_t = torch.sqrt(alpha_cumprod_prev) * pred_x0 + dir_xt\n",
    "        return (x_t.clamp(-1, 1) + 1) / 2\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model, decay):\n",
    "        self.shadow = {k: v.clone().detach() for k, v in model.state_dict().items()}\n",
    "        self.decay = decay\n",
    "    def copy_to(self, model): model.load_state_dict(self.shadow, strict=True)\n",
    "\n",
    "# =================================================================================\n",
    "# 3. Model Architectures\n",
    "# =================================================================================\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm = nn.GroupNorm(8, out_ch)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    def forward(self, x, t):\n",
    "        h = self.norm(self.relu(self.conv1(x)))\n",
    "        h += self.relu(self.time_mlp(t)).unsqueeze(-1).unsqueeze(-1)\n",
    "        return self.dropout(self.norm(self.relu(self.conv2(h))))\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        return torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "\n",
    "class ConditionalUNet(nn.Module):\n",
    "    def __init__(self, in_channels=6, out_channels=3, time_emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down1 = Block(in_channels, 64, time_emb_dim)\n",
    "        self.down2 = Block(64, 128, time_emb_dim)\n",
    "        self.down3 = Block(128, 256, time_emb_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bot1 = Block(256, 512, time_emb_dim)\n",
    "        self.up_trans_1 = nn.ConvTranspose2d(512, 256, 2, 2)\n",
    "        self.up_conv_1 = Block(512, 256, time_emb_dim)\n",
    "        self.up_trans_2 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        self.up_conv_2 = Block(256, 128, time_emb_dim)\n",
    "        self.up_trans_3 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        self.up_conv_3 = Block(128, 64, time_emb_dim)\n",
    "        self.out = nn.Conv2d(64, out_channels, 1)\n",
    "    def forward(self, x, t, condition):\n",
    "        x_cond = torch.cat([x, condition], dim=1)\n",
    "        t_emb = self.time_mlp(t)\n",
    "        h1 = self.down1(x_cond, t_emb)\n",
    "        h2 = self.down2(self.pool(h1), t_emb)\n",
    "        h3 = self.down3(self.pool(h2), t_emb)\n",
    "        bot = self.bot1(self.pool(h3), t_emb)\n",
    "        d1 = self.up_conv_1(torch.cat([self.up_trans_1(bot), h3], dim=1), t_emb)\n",
    "        d2 = self.up_conv_2(torch.cat([self.up_trans_2(d1), h2], dim=1), t_emb)\n",
    "        d3 = self.up_conv_3(torch.cat([self.up_trans_3(d2), h1], dim=1), t_emb)\n",
    "        return self.out(d3)\n",
    "\n",
    "# =================================================================================\n",
    "# 4. Image Loading with .NEF Support via Pillow\n",
    "# =================================================================================\n",
    "def load_image(image_path, transform):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load {image_path}. Ensure Pillow supports .nef (try installing 'pillow-heif'). Error: {str(e)}\")\n",
    "    return transform(img)\n",
    "\n",
    "# =================================================================================\n",
    "# 5. Inference\n",
    "# =================================================================================\n",
    "def infer(input_path, output_path, direction='water_to_land'):\n",
    "    # --- Load Models ---\n",
    "    U_w2l = ConditionalUNet().to(cfg.DEVICE)\n",
    "    U_l2w = ConditionalUNet().to(cfg.DEVICE)\n",
    "    \n",
    "    print(f\"üîÑ Loading checkpoint from {cfg.CHECKPOINT_PATH}...\")\n",
    "    ckpt = torch.load(cfg.CHECKPOINT_PATH, map_location=cfg.DEVICE)\n",
    "    U_w2l.load_state_dict(ckpt['U_w2l'])\n",
    "    U_l2w.load_state_dict(ckpt['U_l2w'])\n",
    "    ema_w2l = EMA(U_w2l, cfg.EMA_DECAY)\n",
    "    ema_l2w = EMA(U_l2w, cfg.EMA_DECAY)\n",
    "    ema_w2l.shadow = ckpt['ema_w2l']\n",
    "    ema_l2w.shadow = ckpt['ema_l2w']\n",
    "    \n",
    "    # Use EMA for sampling\n",
    "    ema_U = U_w2l if direction == 'water_to_land' else U_l2w\n",
    "    ema = ema_w2l if direction == 'water_to_land' else ema_l2w\n",
    "    ema.copy_to(ema_U)\n",
    "    \n",
    "    diffusion = Diffusion()\n",
    "    \n",
    "    # --- Prepare Input ---\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((cfg.IMG_SIZE, cfg.IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    condition = load_image(input_path, transform).unsqueeze(0).to(cfg.DEVICE)\n",
    "    \n",
    "    # --- Sample ---\n",
    "    print(\"üì∏ Generating translated image...\")\n",
    "    translated = diffusion.sample(ema_U, n=1, condition_tensor=condition)\n",
    "    \n",
    "    # --- Save ---\n",
    "    from torchvision.utils import save_image\n",
    "    save_image(translated, output_path)\n",
    "    print(f\"‚úÖ Saved output to {output_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hardcoded input path for the specified .nef file\n",
    "    input_path = \"D:\\\\DAIICT\\\\Sem 3\\\\Major Project 1\\\\AG-CycleDiffusion\\\\raw data\\\\IIT Jammu Dataset\\\\Fish4Knowlege\\\\imgs\\\\Crowded\\\\400000117.jpg\"\n",
    "    output_path = os.path.join(cfg.SAMPLE_DIR, 'LFT_5424_translated.png')\n",
    "    direction = 'water_to_land'  # Default direction; change to 'land_to_water' if needed\n",
    "    \n",
    "    infer(input_path, output_path, direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944c546",
   "metadata": {},
   "source": [
    "## F4K Data Generation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6c447d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading checkpoint from D:\\DAIICT\\Sem 3\\Major Project 1\\AG-CycleDiffusion\\checkpoints\\checkpoint_step_60000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aniruddha shinde\\AppData\\Local\\Temp\\ipykernel_16860\\2215779438.py:127: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(args.checkpoint_path, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully.\n",
      "Scanning for images in specified subfolders: ['imgs', 'test imgs'] within D:\\DAIICT\\Sem 3\\Major Project 1\\AG-CycleDiffusion\\raw data\\IIT Jammu Dataset\\Fish4Knowlege...\n",
      " found 687 images to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 687/687 [04:42<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Batch inference complete!\n",
      " results saved to: D:\\DAIICT\\Sem 3\\Major Project 1\\AG-CycleDiffusion\\inferred_samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =================================================================================\n",
    "# 1. Model & Diffusion Architecture (Copied from your script)\n",
    "# =================================================================================\n",
    "\n",
    "# --- Model Architectures ---\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm = nn.GroupNorm(8, out_ch)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    def forward(self, x, t):\n",
    "        h = self.norm(self.relu(self.conv1(x)))\n",
    "        h += self.relu(self.time_mlp(t)).unsqueeze(-1).unsqueeze(-1)\n",
    "        return self.dropout(self.norm(self.relu(self.conv2(h))))\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        return torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "\n",
    "class ConditionalUNet(nn.Module):\n",
    "    def __init__(self, in_channels=6, out_channels=3, time_emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down1 = Block(in_channels, 64, time_emb_dim)\n",
    "        self.down2 = Block(64, 128, time_emb_dim)\n",
    "        self.down3 = Block(128, 256, time_emb_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bot1 = Block(256, 512, time_emb_dim)\n",
    "        self.up_trans_1 = nn.ConvTranspose2d(512, 256, 2, 2)\n",
    "        self.up_conv_1 = Block(512, 256, time_emb_dim)\n",
    "        self.up_trans_2 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        self.up_conv_2 = Block(256, 128, time_emb_dim)\n",
    "        self.up_trans_3 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        self.up_conv_3 = Block(128, 64, time_emb_dim)\n",
    "        self.out = nn.Conv2d(64, out_channels, 1)\n",
    "    def forward(self, x, t, condition):\n",
    "        x_cond = torch.cat([x, condition], dim=1)\n",
    "        t_emb = self.time_mlp(t)\n",
    "        h1 = self.down1(x_cond, t_emb)\n",
    "        h2 = self.down2(self.pool(h1), t_emb)\n",
    "        h3 = self.down3(self.pool(h2), t_emb)\n",
    "        bot = self.bot1(self.pool(h3), t_emb)\n",
    "        d1 = self.up_conv_1(torch.cat([self.up_trans_1(bot), h3], dim=1), t_emb)\n",
    "        d2 = self.up_conv_2(torch.cat([self.up_trans_2(d1), h2], dim=1), t_emb)\n",
    "        d3 = self.up_conv_3(torch.cat([self.up_trans_3(d2), h1], dim=1), t_emb)\n",
    "        return self.out(d3)\n",
    "\n",
    "# --- Diffusion Logic & Helpers ---\n",
    "class EMA:\n",
    "    def __init__(self, model, decay):\n",
    "        self.shadow = {k: v.clone().detach() for k, v in model.state_dict().items()}\n",
    "        self.decay = decay\n",
    "    def copy_to(self, model): model.load_state_dict(self.shadow, strict=True)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008, device=\"cuda\"):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, device=device)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return betas.clamp(1e-6, 0.999)\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, timesteps=200, device=\"cuda\"):\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        self.betas = cosine_beta_schedule(timesteps, device=device)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, n, condition_tensor, n_steps=25):\n",
    "        model.eval()\n",
    "        x_t = torch.randn((n, 3, 256, 256), device=self.device)\n",
    "        ts_vec = torch.linspace(self.timesteps - 1, 0, n_steps + 1).long().to(self.device)\n",
    "        for i in range(n_steps):\n",
    "            t = ts_vec[i].expand(n)\n",
    "            pred_x0 = model(x_t, t, condition_tensor)\n",
    "            \n",
    "            alpha_cumprod = self.alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "            alpha_cumprod_prev = self.alphas_cumprod[ts_vec[i + 1]].view(-1, 1, 1, 1) if ts_vec[i + 1] >= 0 else torch.ones_like(alpha_cumprod)\n",
    "            \n",
    "            pred_noise = (x_t - torch.sqrt(alpha_cumprod) * pred_x0) / torch.sqrt(1. - alpha_cumprod)\n",
    "            dir_xt = torch.sqrt(1. - alpha_cumprod_prev) * pred_noise\n",
    "            x_t = torch.sqrt(alpha_cumprod_prev) * pred_x0 + dir_xt\n",
    "        return (x_t.clamp(-1, 1) + 1) / 2\n",
    "\n",
    "# =================================================================================\n",
    "# 2. Main Batch Inference Logic\n",
    "# =================================================================================\n",
    "def main(args):\n",
    "    \"\"\"\n",
    "    Main function to run batch inference on a folder of images.\n",
    "    \"\"\"\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # --- Load Model Once ---\n",
    "    print(f\"üîÑ Loading checkpoint from {args.checkpoint_path}...\")\n",
    "    U_w2l = ConditionalUNet().to(DEVICE)\n",
    "    U_l2w = ConditionalUNet().to(DEVICE)\n",
    "    \n",
    "    ckpt = torch.load(args.checkpoint_path, map_location=DEVICE)\n",
    "    \n",
    "    # Select the correct model and EMA weights based on direction\n",
    "    if args.direction == 'water_to_land':\n",
    "        model = U_w2l\n",
    "        model.load_state_dict(ckpt['U_w2l'])\n",
    "        ema_weights = ckpt['ema_w2l']\n",
    "    elif args.direction == 'land_to_water':\n",
    "        model = U_l2w\n",
    "        model.load_state_dict(ckpt['U_l2w'])\n",
    "        ema_weights = ckpt['ema_l2w']\n",
    "    else:\n",
    "        raise ValueError(\"Invalid direction. Choose 'water_to_land' or 'land_to_water'.\")\n",
    "        \n",
    "    # Apply EMA weights for better sample quality\n",
    "    ema = EMA(model, decay=0.999) # Decay value doesn't matter here\n",
    "    ema.shadow = ema_weights\n",
    "    ema.copy_to(model)\n",
    "    model.eval()\n",
    "    print(\"‚úÖ Model loaded successfully.\")\n",
    "    \n",
    "    diffusion = Diffusion(device=DEVICE)\n",
    "    \n",
    "    # --- Prepare Image Transformations ---\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # --- Find All Images Recursively from Specific Subfolders ---\n",
    "    image_paths = []\n",
    "    supported_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.nef')\n",
    "    # Define the only folders you want to process\n",
    "    target_subfolders = ['imgs', 'test imgs'] \n",
    "\n",
    "    print(f\"Scanning for images in specified subfolders: {target_subfolders} within {args.input_folder}...\")\n",
    "\n",
    "    for subfolder in target_subfolders:\n",
    "        # Create the full path to the subfolder to start the search\n",
    "        start_path = os.path.join(args.input_folder, subfolder)\n",
    "        \n",
    "        if not os.path.isdir(start_path):\n",
    "            print(f\"‚ö†Ô∏è Warning: Subfolder '{start_path}' not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Walk through the target subfolder and its children\n",
    "        for root, _, files in os.walk(start_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(supported_extensions):\n",
    "                    image_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(\"‚ùå No images found. Check your input folder and specified subfolders.\")\n",
    "        return\n",
    "\n",
    "    print(f\" found {len(image_paths)} images to process.\")\n",
    "    \n",
    "    # --- Process Each Image ---\n",
    "    for img_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "        try:\n",
    "            # --- Determine Output Path and Create Directories ---\n",
    "            relative_path = os.path.relpath(os.path.dirname(img_path), args.input_folder)\n",
    "            output_dir = os.path.join(args.output_folder, relative_path)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "            output_path = os.path.join(output_dir, f\"{filename}.png\")\n",
    "            \n",
    "            # --- Prepare Input Image ---\n",
    "            condition_image = Image.open(img_path).convert(\"RGB\")\n",
    "            condition = transform(condition_image).unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            # --- Generate Translated Image ---\n",
    "            translated = diffusion.sample(model, n=1, condition_tensor=condition, n_steps=args.n_steps)\n",
    "            \n",
    "            # --- Save Output ---\n",
    "            from torchvision.utils import save_image\n",
    "            save_image(translated, output_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not process {img_path}. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\nüéâ Batch inference complete!\")\n",
    "    print(f\" results saved to: {args.output_folder}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- 1. DEFINE YOUR PATHS HERE ---\n",
    "    # IMPORTANT: Use an 'r' before the quotes for Windows paths\n",
    "    input_folder = r\"D:\\DAIICT\\Sem 3\\Major Project 1\\AG-CycleDiffusion\\raw data\\IIT Jammu Dataset\\Fish4Knowlege\"\n",
    "    output_folder = r\"D:\\DAIICT\\Sem 3\\Major Project 1\\AG-CycleDiffusion\\inferred_samples\"\n",
    "    checkpoint_path = r\"D:\\DAIICT\\Sem 3\\Major Project 1\\AG-CycleDiffusion\\checkpoints\\checkpoint_step_60000.pth\"\n",
    "\n",
    "    # --- 2. DO NOT CHANGE THE CODE BELOW ---\n",
    "    parser = argparse.ArgumentParser(description=\"Batch inference script for AG-CycleDiffusion.\")\n",
    "    \n",
    "    parser.add_argument('--input_folder', type=str, required=True,\n",
    "                        help='Path to the root folder containing the target subfolders (e.g., \"imgs\", \"test imgs\").')\n",
    "                        \n",
    "    parser.add_argument('--output_folder', type=str, required=True,\n",
    "                        help='Path to the folder where translated images will be saved.')\n",
    "                        \n",
    "    parser.add_argument('--checkpoint_path', type=str, required=True,\n",
    "                        help='Path to the model checkpoint .pth file.')\n",
    "                        \n",
    "    parser.add_argument('--direction', type=str, default='water_to_land',\n",
    "                        choices=['water_to_land', 'land_to_water'],\n",
    "                        help='Translation direction.')\n",
    "                        \n",
    "    parser.add_argument('--n_steps', type=int, default=25,\n",
    "                        help='Number of diffusion sampling steps.')\n",
    "\n",
    "    # This list simulates passing arguments from the command line\n",
    "    args_list = [\n",
    "        '--input_folder', input_folder,\n",
    "        '--output_folder', output_folder,\n",
    "        '--checkpoint_path', checkpoint_path\n",
    "    ]\n",
    "\n",
    "    args = parser.parse_args(args_list)\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDPM (3.10.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
